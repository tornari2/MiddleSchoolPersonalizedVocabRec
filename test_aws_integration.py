#!/usr/bin/env python3
"""
AWS Integration Test - Comprehensive End-to-End Verification
Tests actual usage patterns to ensure the infrastructure works for the Vocabulary Recommendation Engine.
"""

import boto3
import json
import time
import sys
from datetime import datetime, timezone
from botocore.exceptions import ClientError
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# AWS Resources from Terraform
AWS_REGION = "us-east-1"
INPUT_BUCKET = "vocab-rec-engine-input-data-dev"
OUTPUT_BUCKET = "vocab-rec-engine-output-reports-dev"
PROFILES_TABLE = "vocab-rec-engine-vocabulary-profiles-dev"
RECOMMENDATIONS_TABLE = "vocab-rec-engine-vocabulary-recommendations-dev"
LAMBDA_ROLE_ARN = "arn:aws:iam::971422717446:role/vocab-rec-engine-lambda-execution-role-dev"

class AWSIntegrationTester:
    def __init__(self, region=AWS_REGION):
        try:
            self.session = boto3.Session(region_name=region)
            self.s3 = self.session.client('s3')
            self.dynamodb = self.session.client('dynamodb')
            self.iam = self.session.client('iam')
            logger.info("‚úì AWS clients initialized successfully")
        except Exception as e:
            logger.error(f"‚úó Error initializing AWS clients: {e}")
            sys.exit(1)

    def test_end_to_end_data_flow(self):
        """Test complete data flow: S3 upload ‚Üí DynamoDB storage ‚Üí S3 output"""
        logger.info("\n=== Testing End-to-End Data Flow ===")

        # Sample data mimicking the PRD specification
        sample_student_data = {
            "student_id": "TEST_STUDENT_001",
            "grade_level": 7,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "assignment_type": "writing_assignment",
            "text": "The ecosystem in our local park demonstrates biodiversity through various species of plants and animals. Climate change affects weather patterns globally, requiring immediate action to reduce carbon emissions."
        }

        # Sample vocabulary profile (what Lambda would generate)
        sample_profile = {
            "student_id": "TEST_STUDENT_001",
            "report_date": datetime.now(timezone.utc).date().isoformat(),
            "proficiency_score": 72.5,
            "total_words_analyzed": 45,
            "unique_words_used": 32,
            "grade_level_words_used": 18,
            "vocabulary_complexity_score": 68.3,
            "last_updated": datetime.now(timezone.utc).isoformat()
        }

        # Sample recommendations (what Lambda would generate)
        sample_recommendations = [
            {
                "word": "biodiversity",
                "definition": "The variety of life in the world or in a particular habitat or ecosystem.",
                "context": "Understanding biodiversity helps us appreciate the interconnectedness of all living things.",
                "grade_level": 8,
                "frequency_score": 85,
                "academic_utility": "high"
            },
            {
                "word": "emissions",
                "definition": "The production and discharge of something, especially gas or radiation.",
                "context": "Carbon emissions contribute significantly to global warming.",
                "grade_level": 8,
                "frequency_score": 78,
                "academic_utility": "high"
            }
        ]

        try:
            # Step 1: Upload sample input data to S3 (simulating data ingestion)
            logger.info("üì§ Step 1: Uploading sample student data to S3")
            input_key = f"raw/{sample_student_data['student_id']}/{sample_student_data['timestamp']}.json"

            self.s3.put_object(
                Bucket=INPUT_BUCKET,
                Key=input_key,
                Body=json.dumps(sample_student_data, indent=2),
                ContentType='application/json'
            )
            logger.info(f"‚úì Uploaded input data to s3://{INPUT_BUCKET}/{input_key}")

            # Verify upload
            response = self.s3.get_object(Bucket=INPUT_BUCKET, Key=input_key)
            uploaded_data = json.loads(response['Body'].read().decode('utf-8'))
            if uploaded_data['student_id'] == sample_student_data['student_id']:
                logger.info("‚úì Input data upload verified")
            else:
                raise Exception("Input data verification failed")

            # Step 2: Store vocabulary profile in DynamoDB (simulating processing result)
            logger.info("üóÑÔ∏è  Step 2: Storing vocabulary profile in DynamoDB")

            # Convert to DynamoDB format
            profile_item = {
                'student_id': {'S': sample_profile['student_id']},
                'report_date': {'S': sample_profile['report_date']},
                'proficiency_score': {'N': str(sample_profile['proficiency_score'])},
                'total_words_analyzed': {'N': str(sample_profile['total_words_analyzed'])},
                'unique_words_used': {'N': str(sample_profile['unique_words_used'])},
                'grade_level_words_used': {'N': str(sample_profile['grade_level_words_used'])},
                'vocabulary_complexity_score': {'N': str(sample_profile['vocabulary_complexity_score'])},
                'last_updated': {'S': sample_profile['last_updated']}
            }

            self.dynamodb.put_item(
                TableName=PROFILES_TABLE,
                Item=profile_item
            )
            logger.info(f"‚úì Stored profile for student {sample_profile['student_id']}")

            # Step 3: Store recommendations in DynamoDB
            logger.info("üìä Step 3: Storing vocabulary recommendations in DynamoDB")

            for i, rec in enumerate(sample_recommendations):
                # Create unique timestamp for each recommendation to avoid sort key conflicts
                rec_timestamp = datetime.now(timezone.utc).isoformat().replace(':', '-').replace('.', '-')
                rec_item = {
                    'student_id': {'S': sample_profile['student_id']},
                    'recommendation_date': {'S': f"{sample_profile['report_date']}#{rec_timestamp}"},
                    'word': {'S': rec['word']},
                    'definition': {'S': rec['definition']},
                    'context': {'S': rec['context']},
                    'grade_level': {'N': str(rec['grade_level'])},
                    'frequency_score': {'N': str(rec['frequency_score'])},
                    'academic_utility': {'S': rec['academic_utility']},
                    'recommendation_id': {'S': f"{sample_profile['student_id']}_{i}"}
                }

                self.dynamodb.put_item(
                    TableName=RECOMMENDATIONS_TABLE,
                    Item=rec_item
                )

            logger.info(f"‚úì Stored {len(sample_recommendations)} recommendations")

            # Step 4: Generate and upload report to output S3 bucket
            logger.info("üìÑ Step 4: Generating and uploading student report")

            # Create comprehensive report
            student_report = {
                "student_id": sample_profile['student_id'],
                "report_date": sample_profile['report_date'],
                "proficiency_score": sample_profile['proficiency_score'],
                "summary": {
                    "total_words_analyzed": sample_profile['total_words_analyzed'],
                    "unique_words_used": sample_profile['unique_words_used'],
                    "grade_level_words_used": sample_profile['grade_level_words_used']
                },
                "recommendations": sample_recommendations,
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "input_data_reference": f"s3://{INPUT_BUCKET}/{input_key}"
            }

            output_key = f"reports/{sample_profile['student_id']}/{sample_profile['report_date']}.json"

            self.s3.put_object(
                Bucket=OUTPUT_BUCKET,
                Key=output_key,
                Body=json.dumps(student_report, indent=2),
                ContentType='application/json'
            )
            logger.info(f"‚úì Uploaded report to s3://{OUTPUT_BUCKET}/{output_key}")

            # Step 5: Verify complete data flow by reading back data
            logger.info("üîç Step 5: Verifying complete data flow")

            # Verify profile in DynamoDB
            profile_response = self.dynamodb.get_item(
                TableName=PROFILES_TABLE,
                Key={
                    'student_id': {'S': sample_profile['student_id']},
                    'report_date': {'S': sample_profile['report_date']}
                }
            )

            if 'Item' in profile_response:
                stored_score = float(profile_response['Item']['proficiency_score']['N'])
                if abs(stored_score - sample_profile['proficiency_score']) < 0.01:
                    logger.info("‚úì Profile data verified in DynamoDB")
                else:
                    raise Exception("Profile data verification failed")
            else:
                raise Exception("Profile not found in DynamoDB")

            # Verify recommendations in DynamoDB (scan for all recommendations for this student/date prefix)
            rec_response = self.dynamodb.scan(
                TableName=RECOMMENDATIONS_TABLE,
                FilterExpression='student_id = :sid AND begins_with(recommendation_date, :rdate)',
                ExpressionAttributeValues={
                    ':sid': {'S': sample_profile['student_id']},
                    ':rdate': {'S': sample_profile['report_date']}
                }
            )

            if rec_response['Count'] == len(sample_recommendations):
                logger.info("‚úì Recommendations data verified in DynamoDB")
            else:
                logger.info(f"Found {rec_response['Count']} recommendations, expected {len(sample_recommendations)}")
                # Log what we found for debugging
                for item in rec_response['Items'][:3]:  # Show first 3
                    logger.info(f"  - Found: {item.get('word', {}).get('S', 'unknown')}")
                raise Exception(f"Recommendations count mismatch: expected {len(sample_recommendations)}, got {rec_response['Count']}")

            # Verify report in S3
            report_response = self.s3.get_object(Bucket=OUTPUT_BUCKET, Key=output_key)
            stored_report = json.loads(report_response['Body'].read().decode('utf-8'))

            if (stored_report['student_id'] == sample_profile['student_id'] and
                len(stored_report['recommendations']) == len(sample_recommendations)):
                logger.info("‚úì Report data verified in S3 output bucket")
            else:
                raise Exception("Report data verification failed")

            # Step 6: Clean up test data
            logger.info("üßπ Step 6: Cleaning up test data")

            # Delete from S3
            self.s3.delete_object(Bucket=INPUT_BUCKET, Key=input_key)
            self.s3.delete_object(Bucket=OUTPUT_BUCKET, Key=output_key)

            # Delete from DynamoDB
            self.dynamodb.delete_item(
                TableName=PROFILES_TABLE,
                Key={
                    'student_id': {'S': sample_profile['student_id']},
                    'report_date': {'S': sample_profile['report_date']}
                }
            )

            # Delete recommendations (scan and delete all matching items)
            recs_to_delete = self.dynamodb.scan(
                TableName=RECOMMENDATIONS_TABLE,
                FilterExpression='student_id = :sid AND begins_with(recommendation_date, :rdate)',
                ExpressionAttributeValues={
                    ':sid': {'S': sample_profile['student_id']},
                    ':rdate': {'S': sample_profile['report_date']}
                }
            )

            for item in recs_to_delete['Items']:
                self.dynamodb.delete_item(
                    TableName=RECOMMENDATIONS_TABLE,
                    Key={
                        'student_id': item['student_id'],
                        'recommendation_date': item['recommendation_date']
                    }
                )

            logger.info("‚úì Test data cleaned up successfully")

            return True

        except Exception as e:
            logger.error(f"‚úó End-to-end test failed: {e}")
            return False

    def test_iam_role_policies(self):
        """Test that IAM role has the correct policies attached"""
        logger.info("\n=== Testing IAM Role Policies ===")

        try:
            # Check Lambda execution role policies
            policies_response = self.iam.list_attached_role_policies(
                RoleName="vocab-rec-engine-lambda-execution-role-dev"
            )

            attached_policies = [p['PolicyName'] for p in policies_response['AttachedPolicies']]
            logger.info(f"‚úì Lambda role has {len(attached_policies)} attached policies:")
            for policy in attached_policies:
                logger.info(f"  - {policy}")

            # Check Step Functions role policies
            sf_policies_response = self.iam.list_attached_role_policies(
                RoleName="vocab-rec-engine-step-functions-role-dev"
            )

            sf_attached_policies = [p['PolicyName'] for p in sf_policies_response['AttachedPolicies']]
            logger.info(f"‚úì Step Functions role has {len(sf_attached_policies)} attached policies:")
            for policy in sf_attached_policies:
                logger.info(f"  - {policy}")

            return True

        except Exception as e:
            logger.error(f"‚úó IAM policy test failed: {e}")
            return False

    def test_concurrency_and_scaling(self):
        """Test basic concurrency and scaling readiness"""
        logger.info("\n=== Testing Concurrency and Scaling Readiness ===")

        try:
            # Test DynamoDB table capacity and settings
            profiles_table_info = self.dynamodb.describe_table(TableName=PROFILES_TABLE)
            recs_table_info = self.dynamodb.describe_table(TableName=RECOMMENDATIONS_TABLE)

            # Verify tables are using PAY_PER_REQUEST billing
            profiles_billing = profiles_table_info['Table']['BillingModeSummary']['BillingMode']
            recs_billing = recs_table_info['Table']['BillingModeSummary']['BillingMode']

            if profiles_billing == 'PAY_PER_REQUEST' and recs_billing == 'PAY_PER_REQUEST':
                logger.info("‚úì DynamoDB tables configured for PAY_PER_REQUEST billing (auto-scaling)")
            else:
                logger.warning("‚ö† DynamoDB tables not using PAY_PER_REQUEST billing")

            # Check point-in-time recovery status (may take time to enable after table creation)
            profiles_pitr = profiles_table_info['Table'].get('PointInTimeRecoveryDescription', {}).get('PointInTimeRecoveryStatus')
            recs_pitr = recs_table_info['Table'].get('PointInTimeRecoveryDescription', {}).get('PointInTimeRecoveryStatus')

            if profiles_pitr == 'ENABLED' and recs_pitr == 'ENABLED':
                logger.info("‚úì Point-in-time recovery enabled for data protection")
            elif profiles_pitr is None or recs_pitr is None:
                logger.info("‚Ñπ Point-in-time recovery status not yet available (may take a few minutes to enable)")
            else:
                logger.warning("‚ö† Point-in-time recovery not enabled")

            # Test S3 bucket settings
            input_encryption = self.s3.get_bucket_encryption(Bucket=INPUT_BUCKET)
            output_encryption = self.s3.get_bucket_encryption(Bucket=OUTPUT_BUCKET)

            if (input_encryption and output_encryption and
                input_encryption['ServerSideEncryptionConfiguration']['Rules'][0]['ApplyServerSideEncryptionByDefault']['SSEAlgorithm'] == 'AES256'):
                logger.info("‚úì S3 buckets have server-side encryption enabled")
            else:
                logger.warning("‚ö† S3 bucket encryption not properly configured")

            return True

        except Exception as e:
            logger.error(f"‚úó Concurrency/scaling test failed: {e}")
            return False

    def run_comprehensive_tests(self):
        """Run all comprehensive integration tests"""
        logger.info("üöÄ Starting Comprehensive AWS Integration Tests")
        logger.info(f"Region: {AWS_REGION}")
        logger.info("=" * 60)

        results = {
            "end_to_end_data_flow": self.test_end_to_end_data_flow(),
            "iam_role_policies": self.test_iam_role_policies(),
            "concurrency_scaling": self.test_concurrency_and_scaling()
        }

        logger.info("\n" + "=" * 60)
        logger.info("üìä COMPREHENSIVE TEST RESULTS")

        all_passed = True
        for test_name, passed in results.items():
            status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
            logger.info(f"{test_name.replace('_', ' ').title()}: {status}")
            if not passed:
                all_passed = False

        if all_passed:
            logger.info("\nüéâ ALL INTEGRATION TESTS PASSED!")
            logger.info("Your AWS infrastructure is production-ready and fully functional.")
            logger.info("‚úì End-to-end data flow works correctly")
            logger.info("‚úì IAM permissions are properly configured")
            logger.info("‚úì Concurrency and scaling are ready")
            logger.info("\nYou can confidently proceed with Task 2: Data Generator development!")
        else:
            logger.error("\n‚ö†Ô∏è  SOME INTEGRATION TESTS FAILED")
            logger.info("Please review the errors above and fix any infrastructure issues.")

        return all_passed

def main():
    tester = AWSIntegrationTester()
    success = tester.run_comprehensive_tests()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
